{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9093571,"sourceType":"datasetVersion","datasetId":5487595}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets langchain sentence-transformers torch langchain-huggingface faiss-cpu gradio Flask pyngrok streamlit langchain-community flask_ngrok","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nimport re\nimport threading\nimport torch  # Add this import statement\n\n# Step 1: Load the text file using standard Python\nfile_path = \"/kaggle/input/document/test.txt\"\nwith open(file_path, 'r', encoding='utf-8') as file:\n    text = file.read()\n\n# Step 2: Split the text into chunks using RecursiveCharacterTextSplitter\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ntext_chunks = splitter.split_text(text)\n\n# Step 3: Generate embeddings for each chunk using all-MiniLM-L6-v2 model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device)\nembeddings = model.encode(text_chunks, convert_to_tensor=True, device=device)\n\n# Step 4: Store the embeddings in a FAISS vector database\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)  # Using L2 distance\nindex.add(embeddings.cpu().numpy())\n\n# Step 5: Function to retrieve similar documents based on a user query\ndef retrieve_similar_docs(query, index, model, text_chunks, top_k=3):\n    # Generate embedding for the user query\n    query_embedding = model.encode([query], convert_to_tensor=True, device=device)\n    \n    # Use FAISS to find the nearest neighbors\n    _, indices = index.search(query_embedding.cpu().numpy(), top_k)\n    \n    # Retrieve and return the most similar document chunks\n    similar_docs = [text_chunks[idx] for idx in indices[0]]\n    return similar_docs\n\n# Initialize the model and tokenizer for streaming\nllm_model = AutoModelForCausalLM.from_pretrained(\"TroyDoesAI/Phi-3-Context-Obedient-RAG\")\ntokenizer = AutoTokenizer.from_pretrained(\"TroyDoesAI/Phi-3-Context-Obedient-RAG\")\n\n# Define the prompt template\nprompt_template = \"\"\"\nAct like a professional chatbot for a training institute. Based on the provided documents, answer the following query: {query}\n\nDocuments:\n{documents}\n\nResponse:\n\"\"\"\n\nprompt = PromptTemplate(template=prompt_template, input_variables=[\"query\", \"documents\"])\n\n# Function to stream output\ndef stream_output(prompt_text):\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n    streamer = TextIteratorStreamer(tokenizer, timeout=150.0, skip_prompt=True, skip_special_tokens=True)\n\n    generation_kwargs = {\n        \"inputs\": inputs.input_ids,\n        \"attention_mask\": inputs.attention_mask,\n        \"max_new_tokens\": 250,\n        \"top_k\": 50,\n        \"temperature\": 0.9,\n        \"do_sample\": True\n    }\n\n    generation_thread = threading.Thread(target=llm_model.generate, kwargs=dict(generation_kwargs, streamer=streamer))\n    generation_thread.start()\n\n    for new_text in streamer:\n        print(new_text, end=\"\", flush=True)\n\n# Loop to continuously prompt the user for queries\nwhile True:\n    user_query = input(\"Enter your query (or type 'exit' to stop): \")\n    if user_query.lower() == 'exit':\n        break\n\n    similar_documents = retrieve_similar_docs(user_query, index, model, text_chunks, top_k=5)\n    documents_text = \"\\n\".join(similar_documents)\n    \n    prompt_text = prompt.format(query=user_query, documents=documents_text)\n    \n    stream_output(prompt_text)\n    print(\"\\n\")  # Add a new line after the response is complete\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}